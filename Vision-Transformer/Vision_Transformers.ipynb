{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vision-Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY6svVsdEtUp"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy5NOq1Ct_ER",
        "outputId": "6db54b6a-dbb0-47b2-d7ee-3cd2e71b98a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWVmRk_Kw9KM",
        "outputId": "ee5d0ddb-7aee-47dc-8ae8-419c71c5ce31"
      },
      "source": [
        "mkdir /content/gdrive/MyDrive/vis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/MyDrive/vis’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXKf9Il5YU1L"
      },
      "source": [
        "!pip install -q -U tensorflow-hub tensorflow-text tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EgzT4MvB3PH"
      },
      "source": [
        "import os\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT746Z0PG43i"
      },
      "source": [
        "Downloading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPJhskgsG39n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7abc965-bea9-4d9d-e00d-2f92a0747bbe"
      },
      "source": [
        "root_dir=\"datasets\"\n",
        "annotations_dir=os.path.join(root_dir,\"annotations\")\n",
        "images_dir=os.path.join(root_dir,\"train2014\")\n",
        "tfrecords_dir =os.path.join(root_dir,\"tfrecords\")\n",
        "annotation_file=os.path.join(annotations_dir,\"captions_train2014.json\")\n",
        "\n",
        "\n",
        "#Download images file\n",
        "\n",
        "if not os.path.exists(images_dir):\n",
        "  images_zip=tf.keras.utils.get_file(\n",
        "      \"train2014.zip\",\n",
        "      cache_dir=os.path.abspath(\".\"),\n",
        "      origin=\"http://images.cocodataset.org/zips/train2014.zip\",\n",
        "      extract=True\n",
        "                                 )\n",
        "  os.remove(images_zip)\n",
        "\n",
        "\n",
        "#Download captions\n",
        "if not os.path.exists(annotations_dir):\n",
        "  annotationszip=tf.keras.utils.get_file(\n",
        "      \"captions.zip\",\n",
        "       cache_dir=os.path.abspath(\".\"),\n",
        "       origin=\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
        "       extract=True\n",
        "                     )\n",
        "  os.remove(annotationszip)\n",
        "\n",
        "print(\"Datasets Downloaded Sucessfully\")\n",
        "\n",
        "with open(annotation_file, \"r\") as f:\n",
        "  annotations=json.load(f)[\"annotations\"]\n",
        "\n",
        "image_path_to_caption= collections.defaultdict(list)\n",
        "for element in annotations:\n",
        "  caption=f\"{element['caption'].lower().rstrip('.')}\"\n",
        "  image_path=images_dir+\"/COCO_train2014_\" + \"%012d.jpg\"%(element[\"image_id\"])\n",
        "  image_path_to_caption[image_path].append(caption)\n",
        "\n",
        "image_paths=list(image_path_to_caption.keys())\n",
        "print(f\"Number of images: {len(image_paths)}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510574080/13510573713 [==============================] - 310s 0us/step\n",
            "Datasets Downloaded Sucessfully\n",
            "Number of images: 82783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CC4NIp59Z2o"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgpqZJ329aGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7515aba-4f4e-412a-c169-8a7f4a61f67c"
      },
      "source": [
        "train_size=40000\n",
        "valid_size=5000\n",
        "captions_per_image=2\n",
        "images_per_file=2000\n",
        "\n",
        "train_image_paths=image_paths[:train_size]\n",
        "num_train_files=int(np.ceil(train_size / images_per_file))\n",
        "train_files_prefix = os.path.join(tfrecords_dir,\"train\")\n",
        "\n",
        "valid_image_paths=image_paths[-valid_size:]\n",
        "num_valid_files=int(np.ceil(valid_size / images_per_file))\n",
        "valid_files_prefix = os.path.join(tfrecords_dir, \"valid\")\n",
        "\n",
        "tf.io.gfile.makedirs(tfrecords_dir)\n",
        "\n",
        "\n",
        "def bytes_feature(value):\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def create_example(image_path,caption):\n",
        "  feature ={\n",
        "      \"caption\": bytes_feature(caption.encode()),\n",
        "      \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n",
        "\n",
        "  }\n",
        "\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "\n",
        "def write_tfrecords(file_name,image_paths):\n",
        "  caption_list=[]\n",
        "  image_path_list=[]\n",
        "  for image_path in image_paths:\n",
        "    captions=image_path_to_caption[image_path][:captions_per_image]\n",
        "    caption_list.extend(captions)\n",
        "    image_path_list.extend([image_path]*len(captions))    \n",
        "\n",
        "  with tf.io.TFRecordWriter(file_name) as writer:\n",
        "    for example_idx in range(len(image_path_list)):\n",
        "      example=create_example(\n",
        "          image_path_list[example_idx],\n",
        "          caption_list[example_idx] )  \n",
        "      writer.write(example.SerializeToString())\n",
        "  return example_idx +1         \n",
        "  \n",
        "def write_data(image_paths, num_files, files_prefix):\n",
        "    example_counter = 0\n",
        "    for file_idx in tqdm(range(num_files)):\n",
        "        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n",
        "        start_idx = images_per_file * file_idx\n",
        "        end_idx = start_idx + images_per_file\n",
        "        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n",
        "    return example_counter\n",
        "\n",
        "train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n",
        "print(f\"{train_example_count} training examples were written to tfrecord files.\")\n",
        "\n",
        "valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n",
        "print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [05:16<00:00, 15.81s/it]\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "80000 training examples were written to tfrecord files.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:41<00:00, 13.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10000 evaluation examples were written to tfrecord files.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdUZJ8U7i5ix"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li3xiQdXi5vc"
      },
      "source": [
        "feature_description = {\n",
        "    \"caption\":tf.io.FixedLenFeature([],tf.string),\n",
        "    \"raw_image\":tf.io.FixedLenFeature([],tf.string),\n",
        "}\n",
        "\n",
        "def read_example(example):\n",
        "    features = tf.io.parse_single_example(example,feature_description)\n",
        "    raw_image =features.pop(\"raw_image\")\n",
        "    features[\"image\"] =tf.image.resize(tf.image.decode_jpeg(raw_image,channels=3),size=(299,299))\n",
        "    return features\n",
        "\n",
        "\n",
        "def get_dataset(file_pattern,batch_size):\n",
        "\n",
        "  return (tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n",
        "        .map(\n",
        "            read_example,\n",
        "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        "            deterministic=False,\n",
        "        )\n",
        "        .shuffle(batch_size*10)\n",
        "        .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "        .batch(batch_size)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9swW2j3KppNP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7RsDP6-ppav"
      },
      "source": [
        "def project_embeddings(\n",
        "    embeddings,num_projection_layers,projection_dims,dropout_rate\n",
        "):\n",
        "    projected_embeddings=layers.Dense(units=projection_dims)(embeddings)\n",
        "    for i in range(num_projection_layers):\n",
        "      x = tf.nn.gelu(projected_embeddings)\n",
        "      x = layers.Dense(projection_dims)(x)\n",
        "      x = layers.Dropout(dropout_rate)(x)\n",
        "      x = layers.Add()([projected_embeddings,x])\n",
        "      projected_embeddings = layers.LayerNormalization()(x)\n",
        "    return projected_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09PUWnrC0q72"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7_yksWc0rHH"
      },
      "source": [
        "def create_vision_encoder(\n",
        "    num_projection_layers,projection_dims, dropout_rate ,trainable=False\n",
        "):\n",
        "\n",
        "   xception=keras.applications.Xception(\n",
        "       include_top=False, weights=\"imagenet\",pooling=\"avg\"\n",
        "   )\n",
        "   \n",
        "   for layer in xception.layers:\n",
        "     layer.trainable = trainable\n",
        "      \n",
        "   inputs =layers.Input(shape=(299,299,3), name=\"image_input\")\n",
        "\n",
        "   xception_input =tf.keras.applications.xception.preprocess_input(inputs)\n",
        "\n",
        "   embeddings = xception(xception_input)\n",
        "\n",
        "   outputs =project_embeddings(\n",
        "        embeddings,num_projection_layers, projection_dims,dropout_rate\n",
        "    )\n",
        "\n",
        "   return keras.Model(inputs,outputs,name=\"vision_encoder\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esjK71QNEr8y"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf7dhUweEwW9"
      },
      "source": [
        "def create_text_encoder(\n",
        "    num_projection_layers,projection_dims,dropout_rate,trainable=False\n",
        "):\n",
        "     preprocess=hub.KerasLayer(\n",
        "         \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
        "         name=\"text_preprocessing\",\n",
        "     )\n",
        "\n",
        "     bert = hub.KerasLayer(\n",
        "         \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
        "         \"bert\",\n",
        "     )\n",
        "\n",
        "     bert.trainable =trainable\n",
        "\n",
        "     inputs=layers.Input(shape=(),dtype=tf.string,name=\"text_input\")\n",
        "\n",
        "     bert_inputs =preprocess(inputs)\n",
        "\n",
        "     embeddings =bert(bert_inputs)[\"pooled_output\"]\n",
        "\n",
        "     outputs =project_embeddings(\n",
        "         embeddings,num_projection_layers,projection_dims,dropout_rate\n",
        "         )\n",
        "     \n",
        "     return keras.Model(inputs,outputs,name=\"text_encoder\")\n",
        "\n",
        "     \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kigDbrf2aNDn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPoKfpGcaNL3"
      },
      "source": [
        "class DualEncoder(keras.Model):\n",
        "   def __init__(self,text_encoder,image_encoder,temperature=1.0,**kwargs):\n",
        "     super(DualEncoder,self).__init__(**kwargs)\n",
        "     self.text_encoder=text_encoder\n",
        "     self.image_encoder=image_encoder\n",
        "     self.temperature =temperature\n",
        "     self.loss_tracker=keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "\n",
        "   @property\n",
        "   def metrics(self):\n",
        "       return [self.loss_tracker]\n",
        "\n",
        "   def call(self, features ,training=False):\n",
        "       with tf.device(\"/gpu:0\"):\n",
        "         caption_embeddings=text_encoder(features[\"caption\"],training = training)\n",
        "       \n",
        "       with tf.device(\"/gpu:0\"):\n",
        "         image_embeddings =vision_encoder(features[\"image\"],training=training)\n",
        "        \n",
        "       return caption_embeddings,image_embeddings\n",
        "\n",
        "   def compute_loss(self,caption_embeddings,image_embeddings):\n",
        "        \n",
        "        logits=(\n",
        "            tf.matmul(caption_embeddings,image_embeddings,transpose_b=True)\n",
        "        )\n",
        "\n",
        "        images_similarity=tf.matmul(image_embeddings,image_embeddings,transpose_b=True\n",
        "            \n",
        "        )\n",
        "        caption_similarity=tf.matmul(caption_embeddings,caption_embeddings,transpose_b=True)\n",
        "\n",
        "        targets=keras.activations.softmax((caption_similarity+images_similarity)/(2*self.temperature))\n",
        "\n",
        "        captions_loss= keras.losses.categorical_crossentropy(y_true=targets,y_pred=logits,from_logits=True)        \n",
        "  \n",
        "        images_loss=keras.losses.categorical_crossentropy(y_true=tf.transpose(targets),y_pred=tf.transpose(logits),from_logits=True)\n",
        "         \n",
        "        return (captions_loss+images_loss)/2\n",
        "\n",
        "   def train_step(self,features):\n",
        "       with tf.GradientTape() as tape:\n",
        "         caption_embeddings,image_embeddings = self(features,training=True)\n",
        "         loss=self.compute_loss(caption_embeddings,image_embeddings)\n",
        "       \n",
        "       gradients=tape.gradient(loss,self.trainable_variables)\n",
        "       self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
        "\n",
        "       self.loss_tracker.update_state(loss)\n",
        "\n",
        "       return {\"loss\":self.loss_tracker.result()}\n",
        "\n",
        "   def test_step(self,features):\n",
        "        caption_embeddings,image_embeddings=self(features,training=False) \n",
        "        loss=self.compute_loss(caption_embeddings,image_embeddings)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\":self.loss_tracker.result()}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mHWxqsjouDP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSaUh5ryqQXA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMilcOvNvLeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e32cf2-70f6-4320-fc6f-6cb7d21299f0"
      },
      "source": [
        "num_epochs=14\n",
        "batch_size=256\n",
        "\n",
        "vision_encoder =create_vision_encoder(\n",
        "    num_projection_layers=1,projection_dims=256,dropout_rate=0.1\n",
        ")\n",
        "\n",
        "text_encoder=create_text_encoder(\n",
        "    num_projection_layers=1,projection_dims=256,dropout_rate=0.1\n",
        ")\n",
        "\n",
        "dual_encoder=DualEncoder(text_encoder,vision_encoder,temperature=0.05)\n",
        "\n",
        "dual_encoder.compile(\n",
        "    optimizer=tfa.optimizers.AdamW(learning_rate=0.008,weight_decay=0.001)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lLtI7JBvVqh",
        "outputId": "fca832ff-b676-4a1f-ffac-3b6990174fc3"
      },
      "source": [
        "print(f\"Number of GPUs:{len(tf.config.list_physical_devices('GPU'))}\")\n",
        "print(f\"Number of examples (caption-image pairs):{train_example_count}\")\n",
        "print(f\"Batch size:{batch_size}\")\n",
        "print(f\"Steps per epoch:{int(np.ceil(train_example_count/batch_size))}\")\n",
        "\n",
        "train_dataset = get_dataset(os.path.join(tfrecords_dir,\"train-*.tfrecord\"),batch_size)\n",
        "valid_dataset = get_dataset(os.path.join(tfrecords_dir,\"valid-*.tfrecord\"),batch_size)\n",
        "\n",
        "reduce_lr=keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",factor=0.2,patience=3\n",
        ")\n",
        "\n",
        "early_stopping=tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",patience=5,restore_best_weights=True\n",
        ")\n",
        "\n",
        "history=dual_encoder.fit(train_dataset,\n",
        "                         epochs=num_epochs,\n",
        "                         validation_data=valid_dataset,\n",
        "                         callbacks=[reduce_lr,early_stopping],\n",
        "                         )\n",
        "\n",
        "print(\"Training Completed.Saving vision and text encoders...\")\n",
        "\n",
        "vision_encoder.save(\"vision_encoder\")\n",
        "text_encoder.save(\"text_encoder\")\n",
        "print(\"models are saved\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of GPUs:1\n",
            "Number of examples (caption-image pairs):80000\n",
            "Batch size:256\n",
            "Steps per epoch:313\n",
            "Epoch 1/14\n",
            "313/313 [==============================] - 1611s 5s/step - loss: 6.0138 - val_loss: 5.3356\n",
            "Epoch 2/14\n",
            "313/313 [==============================] - 1588s 5s/step - loss: 4.2459 - val_loss: 4.7084\n",
            "Epoch 3/14\n",
            "313/313 [==============================] - 1596s 5s/step - loss: 3.0900 - val_loss: 3.8823\n",
            "Epoch 4/14\n",
            "313/313 [==============================] - 1557s 5s/step - loss: 2.7397 - val_loss: 3.6749\n",
            "Epoch 5/14\n",
            "313/313 [==============================] - 1561s 5s/step - loss: 2.6056 - val_loss: 3.7301\n",
            "Epoch 6/14\n",
            "313/313 [==============================] - 1563s 5s/step - loss: 2.5168 - val_loss: 3.7737\n",
            "Epoch 7/14\n",
            "313/313 [==============================] - 1544s 5s/step - loss: 2.4506 - val_loss: 3.7461\n",
            "Epoch 8/14\n",
            "313/313 [==============================] - 1536s 5s/step - loss: 2.1070 - val_loss: 3.5905\n",
            "Epoch 9/14\n",
            "313/313 [==============================] - 1533s 5s/step - loss: 2.0825 - val_loss: 3.6230\n",
            "Epoch 10/14\n",
            "313/313 [==============================] - 1539s 5s/step - loss: 2.1207 - val_loss: 3.6187\n",
            "Epoch 11/14\n",
            "313/313 [==============================] - 1538s 5s/step - loss: 2.1085 - val_loss: 3.6820\n",
            "Epoch 12/14\n",
            "313/313 [==============================] - 1536s 5s/step - loss: 1.9589 - val_loss: 3.5098\n",
            "Epoch 13/14\n",
            "313/313 [==============================] - 1536s 5s/step - loss: 1.9989 - val_loss: 3.5169\n",
            "Epoch 14/14\n",
            "313/313 [==============================] - 1529s 5s/step - loss: 2.1348 - val_loss: 3.5330\n",
            "Training Completed.Saving vision and text encoders...\n",
            "INFO:tensorflow:Assets written to: vision_encoder/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: vision_encoder/assets\n",
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 310). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 310). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: text_encoder/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: text_encoder/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "models are saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1oLFpkeoy8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "8dc3dc58-b0ac-45e5-aa5c-58d0aca9b9f3"
      },
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnt_mgwivetG"
      },
      "source": [
        "cp -r vision_encoder /content/gdrive/MyDrive/Vision_encoder4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jqvqmm6vc-x"
      },
      "source": [
        "cp -r text_encoder /content/gdrive/MyDrive/text_encoder4"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}